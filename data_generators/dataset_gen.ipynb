{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images copied to /glb/hou/pt.sgs/data/ml_ai_us/uspcjc/datasets/topsalt/deeplabv3/images\n",
      "Labels copied to /glb/hou/pt.sgs/data/ml_ai_us/uspcjc/datasets/topsalt/deeplabv3/labels\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define the paths to your folders\n",
    "base_path = '/glb/hou/pt.sgs/data/ml_ai_us/uspcjc/datasets/topsalt/wsher'\n",
    "subfolders = ['train', 'val', 'test']\n",
    "save_dir = '/glb/hou/pt.sgs/data/ml_ai_us/uspcjc/datasets/topsalt/deeplabv3'\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "output_image_dir = os.path.join(save_dir, 'images')\n",
    "output_label_dir = os.path.join(save_dir, 'labels')\n",
    "\n",
    "os.makedirs(output_image_dir, exist_ok=True)\n",
    "os.makedirs(output_label_dir, exist_ok=True)\n",
    "\n",
    "# Iterate through each subfolder (train, val, test)\n",
    "for subfolder in subfolders:\n",
    "    image_dir = os.path.join(base_path, 'images', subfolder)\n",
    "    label_dir = os.path.join(base_path, 'labels', subfolder)\n",
    "    \n",
    "    # Get all image files in the current subfolder\n",
    "    image_files = [f for f in os.listdir(image_dir) if f.endswith('.jpg')]\n",
    "    \n",
    "    # Copy image files to the output images folder\n",
    "    for image_file in image_files:\n",
    "        src_image_path = os.path.join(image_dir, image_file)\n",
    "        dst_image_path = os.path.join(output_image_dir, image_file)\n",
    "        shutil.copyfile(src_image_path, dst_image_path)\n",
    "    \n",
    "    # Get all label files in the current subfolder\n",
    "    label_files = [f for f in os.listdir(label_dir) if f.endswith('.png')]\n",
    "    \n",
    "    # Copy label files to the output labels folder\n",
    "    for label_file in label_files:\n",
    "        src_label_path = os.path.join(label_dir, label_file)\n",
    "        dst_label_path = os.path.join(output_label_dir, label_file)\n",
    "        shutil.copyfile(src_label_path, dst_label_path)\n",
    "\n",
    "print(f\"Images copied to {output_image_dir}\")\n",
    "print(f\"Labels copied to {output_label_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.2581, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "x = torch.randn(10, 19, requires_grad=True, device='cuda')\n",
    "y = torch.randint(0, 19, (10,), device='cuda')\n",
    "y[0] = 255\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
    "loss = criterion(x, y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice Loss: 0.3579835891723633\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, logit, target):\n",
    "        n, c, h, w = logit.size()\n",
    "        \n",
    "        # Apply sigmoid to get probabilities for the binary class\n",
    "        logit = torch.sigmoid(logit)\n",
    "        \n",
    "        # Flatten logits and labels\n",
    "        logit_flat = logit.view(n, -1)\n",
    "        target_flat = target.view(n, -1)\n",
    "        \n",
    "        # Calculate intersection and union\n",
    "        intersection = torch.sum(logit_flat * target_flat, dim=1)\n",
    "        union = torch.sum(logit_flat, dim=1) + torch.sum(target_flat, dim=1) + 1e-8\n",
    "        \n",
    "        # Calculate Dice coefficient for the batch\n",
    "        dice_coefficients = (2.0 * intersection / union)\n",
    "        \n",
    "        # Dice Loss is 1 - average Dice coefficient for the batch\n",
    "        dice_loss = 1.0 - dice_coefficients.mean()\n",
    "        \n",
    "        return dice_loss\n",
    "\n",
    "# Example usage:\n",
    "n_classes = 1  # Binary classification (foreground vs background)\n",
    "\n",
    "# Example tensors: a batch of 2 images, each with size 4x4\n",
    "logit = torch.tensor([[[[0.2, 0.8, 0.1, 0.9], [0.3, 0.7, 0.4, 0.6], [0.5, 0.5, 0.2, 0.8], [0.3, 0.7, 0.4, 0.6]]],\n",
    "                      [[[0.3, 0.7, 0.3, 0.7], [0.2, 0.8, 0.1, 0.9], [0.4, 0.6, 0.3, 0.7], [0.5, 0.5, 0.2, 0.8]]]])\n",
    "\n",
    "target = torch.tensor([[[0, 1, 0, 1], [1, 1, 0, 1], [1, 0, 1, 1], [0, 1, 1, 1]],\n",
    "                       [[1, 0, 1, 1], [0, 1, 1, 0], [1, 1, 0, 0], [1, 1, 1, 0]]])\n",
    "\n",
    "# Initialize DiceLoss and compute the loss\n",
    "dice_loss_fn = DiceLoss()\n",
    "loss = dice_loss_fn(logit, target)\n",
    "print(f\"Dice Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class WeightedDiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(WeightedDiceLoss, self).__init__()\n",
    "        self.weight = weight\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, logit, target):\n",
    "        n, c, h, w = logit.size()\n",
    "        \n",
    "        # Apply sigmoid to get probabilities for the binary class\n",
    "        logit = torch.sigmoid(logit)\n",
    "        \n",
    "        # Flatten logits and labels\n",
    "        logit_flat = logit.view(n, -1)\n",
    "        target_flat = target.view(n, -1)\n",
    "        \n",
    "        # Calculate intersection and union\n",
    "        intersection = torch.sum(logit_flat * target_flat, dim=1)\n",
    "        union = torch.sum(logit_flat, dim=1) + torch.sum(target_flat, dim=1) + 1e-8\n",
    "        \n",
    "        # Calculate Dice coefficient for the batch\n",
    "        dice_coefficients = (2.0 * intersection / union)\n",
    "        \n",
    "        # If weights are provided, apply them to the Dice coefficient\n",
    "        if self.weight is not None:\n",
    "            dice_coefficients = dice_coefficients * self.weight\n",
    "        \n",
    "        # Dice Loss is 1 - average Dice coefficient for the batch\n",
    "        dice_loss = 1.0 - dice_coefficients.mean()\n",
    "        \n",
    "        return dice_loss\n",
    "\n",
    "# Example usage:\n",
    "n_classes = 1  # Binary classification (foreground vs background)\n",
    "\n",
    "# # Example tensors: a batch of 2 images, each with size 4x4\n",
    "# logit = torch.tensor([[[[0.2, 0.8, 0.1, 0.9], [0.3, 0.7, 0.4, 0.6], [0.5, 0.5, 0.2, 0.8], [0.3, 0.7, 0.4, 0.6]]],\n",
    "#                       [[[0.3, 0.7, 0.3, 0.7], [0.2, 0.8, 0.1, 0.9], [0.4, 0.6, 0.3, 0.7], [0.5, 0.5, 0.2, 0.8]]]])\n",
    "\n",
    "# target = torch.tensor([[[0, 1, 0, 1], [1, 1, 0, 1], [1, 0, 1, 1], [0, 1, 1, 1]],\n",
    "#                        [[1, 0, 1, 1], [0, 1, 1, 0], [1, 1, 0, 0], [1, 1, 1, 0]]])\n",
    "\n",
    "# # Initialize WeightedDiceLoss and compute the loss\n",
    "# weight = 1.0  # Example weight to give more importance to the foreground class\n",
    "# dice_loss_fn = WeightedDiceLoss(weight=weight)\n",
    "# loss = dice_loss_fn(logit, target)\n",
    "# print(f\"Weighted Dice Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Dice Loss: 0.3579835891723633\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class WeightedDiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(WeightedDiceLoss, self).__init__()\n",
    "        self.weight = weight\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, logit, target):\n",
    "        n, c, h, w = logit.size()\n",
    "        \n",
    "        # Apply sigmoid to get probabilities for the binary class\n",
    "        logit = torch.sigmoid(logit)\n",
    "        \n",
    "        # Flatten logits and labels\n",
    "        logit_flat = logit.view(n, -1)\n",
    "        target_flat = target.view(n, -1).float()\n",
    "        \n",
    "        # Calculate intersection and union\n",
    "        intersection = torch.sum(logit_flat * target_flat, dim=1)\n",
    "        union = torch.sum(logit_flat, dim=1) + torch.sum(target_flat, dim=1) + 1e-8\n",
    "        \n",
    "        # Calculate Dice coefficient for the batch\n",
    "        dice_coefficients = (2.0 * intersection / union)\n",
    "        \n",
    "        # If weights are provided, apply them to the Dice coefficient\n",
    "        if self.weight is not None:\n",
    "            dice_coefficients = dice_coefficients * self.weight\n",
    "        \n",
    "        # Dice Loss is 1 - average Dice coefficient for the batch\n",
    "        dice_loss = 1.0 - dice_coefficients.mean()\n",
    "        \n",
    "        return dice_loss\n",
    "\n",
    "# Example usage:\n",
    "n_classes = 1  # Binary classification (foreground vs background)\n",
    "\n",
    "# Example tensors: a batch of 2 images, each with size 4x4\n",
    "logit = torch.tensor([[[[0.2, 0.8, 0.1, 0.9], [0.3, 0.7, 0.4, 0.6], [0.5, 0.5, 0.2, 0.8], [0.3, 0.7, 0.4, 0.6]]],\n",
    "                      [[[0.3, 0.7, 0.3, 0.7], [0.2, 0.8, 0.1, 0.9], [0.4, 0.6, 0.3, 0.7], [0.5, 0.5, 0.2, 0.8]]]])\n",
    "\n",
    "target = torch.tensor([[[0, 1, 0, 1], [1, 1, 0, 1], [1, 0, 1, 1], [0, 1, 1, 1]],\n",
    "                       [[1, 0, 1, 1], [0, 1, 1, 0], [1, 1, 0, 0], [1, 1, 1, 0]]])\n",
    "\n",
    "# Add channel dimension to the target tensor\n",
    "target = target.unsqueeze(1)\n",
    "\n",
    "# Initialize WeightedDiceLoss and compute the loss\n",
    "weight = 1.0  # Example weight to give more importance to the foreground class\n",
    "dice_loss_fn = WeightedDiceLoss(weight=weight)\n",
    "loss = dice_loss_fn(logit, target)\n",
    "print(f\"Weighted Dice Loss: {loss.item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn as nn\n",
    "from torch.nn import MSELoss, SmoothL1Loss, L1Loss\n",
    "\n",
    "\n",
    "def flatten(tensor):\n",
    "    \"\"\"Flattens a given tensor such that the channel axis is first.\n",
    "    The shapes are transformed as follows:\n",
    "       (N, C, D, H, W) -> (C, N * D * H * W)\n",
    "    \"\"\"\n",
    "    # number of channels\n",
    "    C = tensor.size(1)\n",
    "    # new axis order\n",
    "    axis_order = (1, 0) + tuple(range(2, tensor.dim()))\n",
    "    # Transpose: (N, C, D, H, W) -> (C, N, D, H, W)\n",
    "    transposed = tensor.permute(axis_order)\n",
    "    # Flatten: (C, N, D, H, W) -> (C, N * D * H * W)\n",
    "    return transposed.contiguous().view(C, -1)\n",
    "\n",
    "def compute_per_channel_dice(input, target, epsilon=1e-6, weight=None):\n",
    "    \"\"\"\n",
    "    Computes DiceCoefficient as defined in https://arxiv.org/abs/1606.04797 given  a multi channel input and target.\n",
    "    Assumes the input is a normalized probability, e.g. a result of Sigmoid or Softmax function.\n",
    "\n",
    "    Args:\n",
    "         input (torch.Tensor): NxCxSpatial input tensor\n",
    "         target (torch.Tensor): NxCxSpatial target tensor\n",
    "         epsilon (float): prevents division by zero\n",
    "         weight (torch.Tensor): Cx1 tensor of weight per channel/class\n",
    "    \"\"\"\n",
    "\n",
    "    # input and target shapes must match\n",
    "    assert input.size() == target.size(), \"'input' and 'target' must have the same shape\"\n",
    "\n",
    "    input = flatten(input)\n",
    "    target = flatten(target)\n",
    "    target = target.float()\n",
    "\n",
    "    # compute per channel Dice Coefficient\n",
    "    intersect = (input * target).sum(-1)\n",
    "    if weight is not None:\n",
    "        intersect = weight * intersect\n",
    "\n",
    "    # here we can use standard dice (input + target).sum(-1) or extension (see V-Net) (input^2 + target^2).sum(-1)\n",
    "    denominator = (input * input).sum(-1) + (target * target).sum(-1)\n",
    "    return 2 * (intersect / denominator.clamp(min=epsilon))\n",
    "\n",
    "\n",
    "class _AbstractDiceLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Base class for different implementations of Dice loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weight=None, normalization='sigmoid'):\n",
    "        super(_AbstractDiceLoss, self).__init__()\n",
    "        self.register_buffer('weight', weight)\n",
    "        # The output from the network during training is assumed to be un-normalized probabilities and we would\n",
    "        # like to normalize the logits. Since Dice (or soft Dice in this case) is usually used for binary data,\n",
    "        # normalizing the channels with Sigmoid is the default choice even for multi-class segmentation problems.\n",
    "        # However if one would like to apply Softmax in order to get the proper probability distribution from the\n",
    "        # output, just specify `normalization=Softmax`\n",
    "        assert normalization in ['sigmoid', 'softmax', 'none']\n",
    "        if normalization == 'sigmoid':\n",
    "            self.normalization = nn.Sigmoid()\n",
    "        elif normalization == 'softmax':\n",
    "            self.normalization = nn.Softmax(dim=1)\n",
    "        else:\n",
    "            self.normalization = lambda x: x\n",
    "\n",
    "    def dice(self, input, target, weight):\n",
    "        # actual Dice score computation; to be implemented by the subclass\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        # get probabilities from logits\n",
    "        input = self.normalization(input)\n",
    "\n",
    "        # compute per channel Dice coefficient\n",
    "        per_channel_dice = self.dice(input, target, weight=self.weight)\n",
    "\n",
    "        # average Dice score across all channels/classes\n",
    "        return 1. - torch.mean(per_channel_dice)\n",
    "\n",
    "\n",
    "class DiceLoss(_AbstractDiceLoss):\n",
    "    \"\"\"Computes Dice Loss according to https://arxiv.org/abs/1606.04797.\n",
    "    For multi-class segmentation `weight` parameter can be used to assign different weights per class.\n",
    "    The input to the loss function is assumed to be a logit and will be normalized by the Sigmoid function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weight=None, normalization='sigmoid'):\n",
    "        super().__init__(weight, normalization)\n",
    "\n",
    "    def dice(self, input, target, weight):\n",
    "        return compute_per_channel_dice(input, target, weight=self.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice Loss: 0.3459131717681885\n"
     ]
    }
   ],
   "source": [
    "N, C, D, H, W = 2, 3, 4, 5, 6  # Example dimensions: Batch size, Channels, Depth, Height, Width\n",
    "input_tensor = torch.randn(N, C, D, H, W, requires_grad=True)  # Random logits\n",
    "target_tensor = torch.randint(0, 2, (N, C, D, H, W)).float()   # Binary target tensor\n",
    "\n",
    "# Step 2: Initialize DiceLoss class\n",
    "dice_loss = DiceLoss(normalization='sigmoid')\n",
    "\n",
    "# Step 3: Compute loss\n",
    "loss = dice_loss(input_tensor, target_tensor)\n",
    "\n",
    "# Print the result\n",
    "print(\"Dice Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice Loss: 0.21425658464431763\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "# Assuming the previous code defining flatten, compute_per_channel_dice, _AbstractDiceLoss, and DiceLoss is in the same script or imported correctly\n",
    "\n",
    "# Define the logit and target tensors as provided\n",
    "logit = torch.tensor([[[[0.2, 0.8, 0.1, 0.9], \n",
    "                        [0.3, 0.7, 0.4, 0.6], \n",
    "                        [0.5, 0.5, 0.2, 0.8], \n",
    "                        [0.3, 0.7, 0.4, 0.6]]],\n",
    "\n",
    "                      [[[0.3, 0.7, 0.3, 0.7], \n",
    "                        [0.2, 0.8, 0.1, 0.9], \n",
    "                        [0.4, 0.6, 0.3, 0.7], \n",
    "                        [0.5, 0.5, 0.2, 0.8]]]])\n",
    "\n",
    "target = torch.tensor([[[0, 1, 0, 1], \n",
    "                        [1, 1, 0, 1], \n",
    "                        [1, 0, 1, 1], \n",
    "                        [0, 1, 1, 1]],\n",
    "\n",
    "                       [[1, 0, 1, 1], \n",
    "                        [0, 1, 1, 0], \n",
    "                        [1, 1, 0, 0], \n",
    "                        [1, 1, 1, 0]]])\n",
    "\n",
    "# Add a channel dimension to target to match the expected shape (N, C, H, W)\n",
    "target = target.unsqueeze(1).float()\n",
    "\n",
    "# Initialize the DiceLoss class\n",
    "dice_loss = DiceLoss(normalization='sigmoid')\n",
    "\n",
    "# Compute the loss\n",
    "loss = dice_loss(logit, target)\n",
    "\n",
    "# Print the result\n",
    "print(\"Dice Loss:\", loss.item())\n",
    "\n",
    "# # Optional: Backward pass to check gradient computation\n",
    "# loss.backward()\n",
    "# print(\"Gradients:\", logit.grad)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dino",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
